---
phase: 06-csv-and-convenience-helpers
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - bindings/python/src/quiver/database.py
  - bindings/python/tests/test_database_read_scalar.py
  - bindings/python/tests/test_database_read_vector.py
  - bindings/python/tests/test_database_read_set.py
autonomous: true
requirements:
  - CONV-01
  - CONV-02
  - CONV-03
  - CONV-04
  - CONV-05

must_haves:
  truths:
    - "read_all_scalars_by_id returns a dict of all scalar attribute values including id and label"
    - "read_all_vectors_by_id returns a dict of group names to typed lists for single-column vector groups"
    - "read_all_sets_by_id returns a dict of group names to typed lists for single-column set groups"
    - "read_scalar_date_time_by_id returns a timezone-aware UTC datetime for valid ISO strings"
    - "read_vector_date_time_by_id returns a list of timezone-aware UTC datetimes"
    - "read_set_date_time_by_id returns a list of timezone-aware UTC datetimes"
    - "read_vector_group_by_id returns list of row dicts with vector_index included for multi-column vector groups"
    - "read_set_group_by_id returns list of row dicts for multi-column set groups"
    - "DATE_TIME columns in read_all_* and group readers are parsed to datetime objects"
  artifacts:
    - path: "bindings/python/src/quiver/database.py"
      provides: "_parse_datetime helper, datetime helpers, read_all_* helpers, group readers"
      contains: "_parse_datetime"
    - path: "bindings/python/tests/test_database_read_scalar.py"
      provides: "Tests for read_all_scalars_by_id and read_scalar_date_time_by_id"
    - path: "bindings/python/tests/test_database_read_vector.py"
      provides: "Tests for read_all_vectors_by_id, read_vector_date_time_by_id, read_vector_group_by_id"
    - path: "bindings/python/tests/test_database_read_set.py"
      provides: "Tests for read_all_sets_by_id, read_set_date_time_by_id, read_set_group_by_id"
  key_links:
    - from: "database.py:read_all_scalars_by_id"
      to: "database.py:list_scalar_attributes + read_scalar_*_by_id"
      via: "type dispatch on attr.data_type"
      pattern: "list_scalar_attributes.*read_scalar"
    - from: "database.py:read_vector_group_by_id"
      to: "database.py:get_vector_metadata + read_vector_*_by_id"
      via: "metadata column iteration + type dispatch"
      pattern: "get_vector_metadata.*read_vector"
    - from: "database.py:_parse_datetime"
      to: "datetime.fromisoformat + replace(tzinfo=timezone.utc)"
      via: "stdlib datetime parsing"
      pattern: "fromisoformat.*timezone.utc"
---

<objective>
Add pure-Python convenience helpers to the Database class: DateTime parsing helpers, composite read-all helpers, and multi-column group readers. All compose existing binding methods with no new FFI calls.

Purpose: Provides high-level single-call read operations matching Julia/Dart convenience API surface.
Output: 8 new Database methods, 1 module-level datetime parser, and tests for all convenience helpers.
</objective>

<execution_context>
@C:/Users/rsampaio/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/rsampaio/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-csv-and-convenience-helpers/06-RESEARCH.md

@bindings/python/src/quiver/database.py
@bindings/python/tests/conftest.py
@bindings/python/tests/test_database_read_scalar.py
@bindings/python/tests/test_database_read_vector.py
@bindings/python/tests/test_database_read_set.py
@bindings/julia/src/database_read.jl
@tests/schemas/valid/basic.sql
@tests/schemas/valid/collections.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add DateTime helpers and composite read-all methods</name>
  <files>
    bindings/python/src/quiver/database.py
  </files>
  <action>
1. Add module-level `_parse_datetime` helper function to `database.py`, before the `_marshal_params` function (in the module-level helpers section):

```python
from datetime import datetime, timezone

def _parse_datetime(s: str | None) -> datetime | None:
    """Parse an ISO 8601 datetime string to timezone-aware UTC datetime.

    Returns None if input is None. Raises ValueError on malformed input.
    Both 'YYYY-MM-DDTHH:MM:SS' and 'YYYY-MM-DD HH:MM:SS' formats are supported.
    """
    if s is None:
        return None
    return datetime.fromisoformat(s).replace(tzinfo=timezone.utc)
```

Note: The existing `from datetime import datetime` at the top of the file should be updated to `from datetime import datetime, timezone`.

2. Update the existing `query_date_time` method to use `_parse_datetime` instead of inline `datetime.fromisoformat()`. This ensures consistent UTC timezone-aware behavior per user decision. Change:
```python
return datetime.fromisoformat(result)
```
to:
```python
return _parse_datetime(result)
```

3. Add 3 DateTime helper methods to the Database class. Place after `query_date_time`:

```python
def read_scalar_date_time_by_id(
    self, collection: str, attribute: str, id: int,
) -> datetime | None:
    """Read a datetime scalar attribute. Returns timezone-aware UTC datetime or None."""
    return _parse_datetime(self.read_scalar_string_by_id(collection, attribute, id))

def read_vector_date_time_by_id(
    self, collection: str, attribute: str, id: int,
) -> list[datetime]:
    """Read datetime values from a vector. Returns list of timezone-aware UTC datetimes."""
    return [_parse_datetime(s) for s in self.read_vector_strings_by_id(collection, attribute, id)]

def read_set_date_time_by_id(
    self, collection: str, attribute: str, id: int,
) -> list[datetime]:
    """Read datetime values from a set. Returns list of timezone-aware UTC datetimes."""
    return [_parse_datetime(s) for s in self.read_set_strings_by_id(collection, attribute, id)]
```

4. Add `read_all_scalars_by_id` method. Place in a new "Convenience helpers" section after the time series files section:

```python
# -- Convenience helpers ---------------------------------------------------

def read_all_scalars_by_id(self, collection: str, id: int) -> dict:
    """Read all scalar attribute values for an element.

    Returns dict mapping attribute names to typed values.
    Includes id and label. DATE_TIME attributes are parsed to datetime objects.
    """
    self._ensure_open()
    result = {}
    for attr in self.list_scalar_attributes(collection):
        name = attr.name
        if attr.data_type == 0:    # INTEGER
            result[name] = self.read_scalar_integer_by_id(collection, name, id)
        elif attr.data_type == 1:  # FLOAT
            result[name] = self.read_scalar_float_by_id(collection, name, id)
        elif attr.data_type == 3:  # DATE_TIME
            result[name] = self.read_scalar_date_time_by_id(collection, name, id)
        else:                      # STRING (2)
            result[name] = self.read_scalar_string_by_id(collection, name, id)
    return result
```

Per research finding and Julia/Dart precedent: include `id` and `label` in the result dict (they are returned by `list_scalar_attributes` as regular scalar attributes).

5. Add `read_all_vectors_by_id` method. Uses first value column's data_type for type dispatch (single-column groups). Multi-column groups should use `read_vector_group_by_id` instead.

```python
def read_all_vectors_by_id(self, collection: str, id: int) -> dict:
    """Read all vector group values for an element (single-column groups).

    Returns dict mapping group names to typed lists.
    DATE_TIME groups are parsed to datetime objects.
    For multi-column groups, use read_vector_group_by_id.
    """
    self._ensure_open()
    result = {}
    for group in self.list_vector_groups(collection):
        name = group.group_name
        dt = group.value_columns[0].data_type if group.value_columns else 2
        if dt == 0:    # INTEGER
            result[name] = self.read_vector_integers_by_id(collection, name, id)
        elif dt == 1:  # FLOAT
            result[name] = self.read_vector_floats_by_id(collection, name, id)
        elif dt == 3:  # DATE_TIME
            result[name] = self.read_vector_date_time_by_id(collection, name, id)
        else:          # STRING (2)
            result[name] = self.read_vector_strings_by_id(collection, name, id)
    return result
```

6. Add `read_all_sets_by_id` method. Same pattern as vectors:

```python
def read_all_sets_by_id(self, collection: str, id: int) -> dict:
    """Read all set group values for an element (single-column groups).

    Returns dict mapping group names to typed lists.
    DATE_TIME groups are parsed to datetime objects.
    For multi-column groups, use read_set_group_by_id.
    """
    self._ensure_open()
    result = {}
    for group in self.list_set_groups(collection):
        name = group.group_name
        dt = group.value_columns[0].data_type if group.value_columns else 2
        if dt == 0:    # INTEGER
            result[name] = self.read_set_integers_by_id(collection, name, id)
        elif dt == 1:  # FLOAT
            result[name] = self.read_set_floats_by_id(collection, name, id)
        elif dt == 3:  # DATE_TIME
            result[name] = self.read_set_date_time_by_id(collection, name, id)
        else:          # STRING (2)
            result[name] = self.read_set_strings_by_id(collection, name, id)
    return result
```

7. Add `read_vector_group_by_id` method. Reads each value column by type, transposes to row dicts. Per user decision, includes `vector_index` as an integer key in each row dict (preserves ordering info):

```python
def read_vector_group_by_id(
    self, collection: str, group: str, id: int,
) -> list[dict]:
    """Read a multi-column vector group as row dicts.

    Each row is a dict mapping column names to typed values.
    Includes 'vector_index' (0-based) for ordering info.
    Rows are returned in vector_index order.
    DATE_TIME columns are parsed to datetime objects.
    """
    self._ensure_open()
    metadata = self.get_vector_metadata(collection, group)
    columns = metadata.value_columns
    if not columns:
        return []

    column_data = {}
    row_count = 0
    for col in columns:
        name = col.name
        if col.data_type == 0:      # INTEGER
            values = self.read_vector_integers_by_id(collection, name, id)
        elif col.data_type == 1:    # FLOAT
            values = self.read_vector_floats_by_id(collection, name, id)
        elif col.data_type == 3:    # DATE_TIME
            values = self.read_vector_date_time_by_id(collection, name, id)
        else:                       # STRING (2)
            values = self.read_vector_strings_by_id(collection, name, id)
        column_data[name] = values
        row_count = len(values)

    return [
        {"vector_index": i, **{name: values[i] for name, values in column_data.items()}}
        for i in range(row_count)
    ]
```

8. Add `read_set_group_by_id` method. Same pattern as vector group but uses set metadata and set reads:

```python
def read_set_group_by_id(
    self, collection: str, group: str, id: int,
) -> list[dict]:
    """Read a multi-column set group as row dicts.

    Each row is a dict mapping column names to typed values.
    DATE_TIME columns are parsed to datetime objects.
    """
    self._ensure_open()
    metadata = self.get_set_metadata(collection, group)
    columns = metadata.value_columns
    if not columns:
        return []

    column_data = {}
    row_count = 0
    for col in columns:
        name = col.name
        if col.data_type == 0:      # INTEGER
            values = self.read_set_integers_by_id(collection, name, id)
        elif col.data_type == 1:    # FLOAT
            values = self.read_set_floats_by_id(collection, name, id)
        elif col.data_type == 3:    # DATE_TIME
            values = self.read_set_date_time_by_id(collection, name, id)
        else:                       # STRING (2)
            values = self.read_set_strings_by_id(collection, name, id)
        column_data[name] = values
        row_count = len(values)

    return [
        {name: values[i] for name, values in column_data.items()}
        for i in range(row_count)
    ]
```

IMPORTANT: This plan modifies `database.py` which is also touched by Plan 01. The modifications are in completely separate sections:
- Plan 01 adds: `export_csv` method (after time series files, before `__repr__`), `_marshal_csv_export_options` function (module-level), and changes the metadata import line.
- Plan 02 adds: `_parse_datetime` (module-level), datetime helpers (after `query_date_time`), convenience helpers section (new section after time series files).
- If executing in parallel, the executor must merge both sets of changes cleanly. The import line change in Plan 01 (`CSVExportOptions` import) and the datetime import change here are both in the import block -- merge both.
  </action>
  <verify>
Run `python -c "from quiver import Database; print([m for m in dir(Database) if 'date_time' in m or 'read_all' in m or 'group_by_id' in m])"` to confirm all new methods exist on the class.
  </verify>
  <done>All 8 convenience methods exist on Database class. _parse_datetime returns timezone-aware UTC datetime. query_date_time updated to use _parse_datetime for consistency.</done>
</task>

<task type="auto">
  <name>Task 2: Add tests for all convenience helpers</name>
  <files>
    bindings/python/tests/test_database_read_scalar.py
    bindings/python/tests/test_database_read_vector.py
    bindings/python/tests/test_database_read_set.py
  </files>
  <action>
Add tests to existing test files (append to each file). Use existing fixtures (`db` for basic schema, `collections_db` for vectors/sets).

1. In `test_database_read_scalar.py`, add:

- **test_read_scalar_date_time_by_id**: Create element with `date_attribute` set to `"2024-01-15T10:30:00"`. Call `read_scalar_date_time_by_id`. Assert result is `datetime(2024, 1, 15, 10, 30, 0, tzinfo=timezone.utc)`. Assert `result.tzinfo is not None`.

- **test_read_scalar_date_time_by_id_space_format**: Test with `"2024-01-15 10:30:00"` (space separator). Assert same datetime result (Python 3.11+ fromisoformat handles both).

- **test_read_scalar_date_time_by_id_none**: Create element with NULL date. Assert `read_scalar_date_time_by_id` returns `None`.

- **test_read_all_scalars_by_id**: Create element with label, integer_attribute, float_attribute, string_attribute, date_attribute. Call `read_all_scalars_by_id("Configuration", id)`. Assert result is a dict with keys including `"id"`, `"label"`, `"integer_attribute"`, `"float_attribute"`, `"string_attribute"`, `"date_attribute"`. Assert types: `id` is int, `label` is str, `integer_attribute` is int, `float_attribute` is float, `date_attribute` is datetime with UTC tzinfo.

2. In `test_database_read_vector.py`, add:

- **test_read_vector_date_time_by_id**: Using `collections_db`. The `collections.sql` schema has `Collection_vector_values` with `value_int` and `value_float` columns (not datetime). Since the basic schema doesn't have datetime vectors, test this using a schema that has datetime columns. ALTERNATIVE: If no datetime vector schema exists, use `read_vector_strings_by_id` to confirm the underlying data is present, then just test the datetime parsing on a string vector would require a DATE_TIME column. Check the `basic.sql` and `collections.sql` schemas -- neither has a datetime vector. In this case, skip the datetime vector test or use the `mixed_time_series.sql` schema. Since the `mixed_time_series.sql` is for time series not vectors, it's simplest to test `read_vector_date_time_by_id` by verifying the method exists and the parsing logic works via `_parse_datetime` unit tests already in the scalar tests.

- **test_read_all_vectors_by_id**: Using `collections_db`. Create element, update vector with values `[10, 20, 30]` for `value_int`. Call `read_all_vectors_by_id("Collection", id)`. Assert result is a dict with key `"values"` (the group name from `Collection_vector_values`). Assert the value is `[10, 20, 30]`.

- **test_read_vector_group_by_id**: Using `collections_db`. Create element, update vectors: `value_int = [1, 2, 3]`, `value_float = [1.1, 2.2, 3.3]`. Call `read_vector_group_by_id("Collection", "values", id)`. Assert result is a list of 3 dicts. Assert `result[0] == {"vector_index": 0, "value_int": 1, "value_float": 1.1}`. Assert `result[2] == {"vector_index": 2, "value_int": 3, "value_float": 3.3}`. Assert every row has `"vector_index"` key with integer type.

- **test_read_vector_group_by_id_empty**: Create element with no vector data. Assert `read_vector_group_by_id` returns `[]`.

3. In `test_database_read_set.py`, add:

- **test_read_all_sets_by_id**: Using `collections_db`. Create element, update set with tags `["alpha", "beta"]`. Call `read_all_sets_by_id("Collection", id)`. Assert result is a dict with key `"tags"` (from `Collection_set_tags`). Assert value contains `"alpha"` and `"beta"`.

- **test_read_set_group_by_id**: Using `collections_db`. The `Collection_set_tags` table has only one value column (`tag`). Create element, update set tags. Call `read_set_group_by_id("Collection", "tags", id)`. Assert result is a list of row dicts. Each dict has key `"tag"`. Assert `{"tag": "alpha"}` is in the result.

- **test_read_set_group_by_id_empty**: Create element with no set data. Assert `read_set_group_by_id` returns `[]`.

All tests should import `from datetime import datetime, timezone` as needed.
  </action>
  <verify>
Run `python -m pytest bindings/python/tests/test_database_read_scalar.py bindings/python/tests/test_database_read_vector.py bindings/python/tests/test_database_read_set.py -v -k "date_time or read_all or group_by_id"` to verify all new tests pass.
  </verify>
  <done>All convenience helper tests pass. DateTime helpers return timezone-aware UTC datetimes. read_all_* return correctly typed dicts. read_*_group_by_id return row dicts matching read_time_series_group format.</done>
</task>

</tasks>

<verification>
1. `python -m pytest bindings/python/tests/test_database_read_scalar.py -v` -- all scalar tests pass including new datetime and read_all tests
2. `python -m pytest bindings/python/tests/test_database_read_vector.py -v` -- all vector tests pass including new read_all, group_by_id tests
3. `python -m pytest bindings/python/tests/test_database_read_set.py -v` -- all set tests pass including new read_all, group_by_id tests
4. `python -m pytest bindings/python/tests/ -v` -- full test suite passes (no regressions from query_date_time change)
</verification>

<success_criteria>
- _parse_datetime returns timezone-aware UTC datetime objects
- read_scalar_date_time_by_id returns datetime or None for NULL values
- read_vector_date_time_by_id and read_set_date_time_by_id return lists of datetimes
- query_date_time now returns timezone-aware UTC datetime (updated from naive)
- read_all_scalars_by_id includes id, label, and all typed scalar values with DATE_TIME parsed
- read_all_vectors_by_id and read_all_sets_by_id return group-name-to-list dicts
- read_vector_group_by_id and read_set_group_by_id return list of row dicts
- vector_index IS included in read_vector_group_by_id row dicts as an integer key (per user decision, preserves ordering info)
- All existing tests continue to pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/06-csv-and-convenience-helpers/06-02-SUMMARY.md`
</output>
