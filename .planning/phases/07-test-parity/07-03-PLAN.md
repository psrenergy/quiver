---
phase: 07-test-parity
plan: 03
type: execute
wave: 2
depends_on:
  - "07-01"
files_modified:
  - bindings/python/tests/conftest.py
  - bindings/python/tests/test_database_read_vector.py
  - bindings/python/tests/test_database_read_set.py
  - bindings/python/tests/test_database_update.py
  - bindings/python/tests/test_database_create.py
  - bindings/python/tests/test_database_csv.py
  - tests/test_lua_runner.cpp
autonomous: true
requirements:
  - TEST-01
  - TEST-02

must_haves:
  truths:
    - "Python has happy-path tests for string vector reads, integer/float set reads, string vector updates, integer/float set updates, and datetime convenience methods"
    - "Lua has happy-path tests for string vector reads, integer/float set operations, and describe"
    - "Python test suite passes with zero failures"
    - "Lua test suite passes via quiver_tests.exe with zero failures"
    - "All 6 test suites (C++, C API, Julia, Dart, Python, Lua) run with zero failures"
    - "Python read tests retain 3-file-per-type structure (read_scalar, read_vector, read_set) per CONTEXT.md decision -- not merged to 1-file-per-area"
  artifacts:
    - path: "bindings/python/tests/conftest.py"
      provides: "all_types_schema_path and all_types_db fixtures for gap-fill tests"
    - path: "bindings/python/tests/test_database_read_vector.py"
      provides: "Gap-fill tests for read_vector_strings, read_vector_strings_by_id, read_vector_date_time_by_id"
    - path: "bindings/python/tests/test_database_read_set.py"
      provides: "Gap-fill tests for read_set_integers, read_set_floats, read_set_integers_by_id, read_set_floats_by_id, read_set_date_time_by_id"
    - path: "bindings/python/tests/test_database_update.py"
      provides: "Gap-fill tests for update_vector_strings, update_set_integers, update_set_floats"
    - path: "tests/test_lua_runner.cpp"
      provides: "Gap-fill tests for Lua string vector reads, integer/float set operations, describe"
  key_links:
    - from: "bindings/python/tests/test_database_read_vector.py"
      to: "tests/schemas/valid/all_types.sql"
      via: "schema_path fixture"
      pattern: "all_types"
    - from: "tests/test_lua_runner.cpp"
      to: "tests/schemas/valid/all_types.sql"
      via: "SCHEMA_PATH macro"
      pattern: 'SCHEMA_PATH\("all_types.sql"\)'
---

<objective>
Fill all identified test gaps in Python and Lua, then run all 6 test suites to confirm zero failures across the entire project.

Purpose: Complete the test parity audit. Python gaps include missing typed read/update operations and datetime convenience methods. Lua gaps mirror the C++ typed operation gaps. The final step validates that every layer passes.

Output: Gap-fill tests in Python and Lua. All 6 test suites pass.
</objective>

<execution_context>
@C:/Users/rsampaio/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/rsampaio/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-test-parity/07-RESEARCH.md
@.planning/phases/07-test-parity/07-01-SUMMARY.md
@tests/schemas/valid/all_types.sql
@bindings/python/tests/test_database_read_vector.py
@bindings/python/tests/test_database_read_set.py
@bindings/python/tests/test_database_update.py
@bindings/python/tests/test_database_create.py
@bindings/python/tests/test_database_csv.py
@bindings/python/tests/conftest.py
@tests/test_lua_runner.cpp
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fill Python test gaps</name>
  <files>
    bindings/python/tests/conftest.py
    bindings/python/tests/test_database_read_vector.py
    bindings/python/tests/test_database_read_set.py
    bindings/python/tests/test_database_update.py
    bindings/python/tests/test_database_create.py
    bindings/python/tests/test_database_csv.py
  </files>
  <action>
Follow the existing Python test patterns: pytest class-based tests, `Database.from_schema(":memory:", schema_path)`, `Element().set(...)`, assert with plain `assert`. Add gap-fill tests to existing files per user decision (do NOT create new files, do NOT merge existing files).

**Step 0: Add `all_types_db` fixture to `bindings/python/tests/conftest.py`.**

Add two fixtures following the same generator pattern as the existing `collections_db` fixture:

```python
@pytest.fixture
def all_types_schema_path(schemas_path: Path) -> Path:
    """Return the path to the all_types test schema."""
    return schemas_path / "valid" / "all_types.sql"


@pytest.fixture
def all_types_db(all_types_schema_path: Path, tmp_path: Path) -> Generator[Database, None, None]:
    """Create a test database with the all_types schema."""
    database = Database.from_schema(str(tmp_path / "all_types.db"), str(all_types_schema_path))
    yield database
    database.close()
```

Use the `all_types_db` fixture in all gap-fill tests below that need the all_types.sql schema.

**Step 1: Add to `bindings/python/tests/test_database_read_vector.py`:**

- `test_read_vector_strings_bulk`: Open all_types.sql, create 2 elements, set string vectors via `update_vector_strings`, call `read_vector_strings("AllTypes", "labels")`, assert correct dict/list results
- `test_read_vector_strings_by_id`: Create element, set string vector, call `read_vector_strings_by_id("AllTypes", 1, "labels")`, assert values. Note: check Python API parameter order -- it may be (collection, id, attribute) per Python convention from Phase 2
- `test_read_vector_date_time_by_id`: Use collections.sql (which has time series with date_time), create element with time series data, test `read_vector_date_time_by_id` if it applies to vector columns. If this method only applies to vectors with DATE_TIME strings, use an appropriate schema. If no suitable vector exists, skip this test and document why.

**Step 2: Add to `bindings/python/tests/test_database_read_set.py`:**

- `test_read_set_integers_bulk`: Open all_types.sql, create 2 elements, set integer sets, call `read_set_integers("AllTypes", "codes")`, assert
- `test_read_set_integers_by_id`: Create element, set integer set, call `read_set_integers_by_id("AllTypes", 1, "codes")`, assert sorted values
- `test_read_set_floats_bulk`: Create 2 elements, set float sets, call `read_set_floats("AllTypes", "weights")`, assert
- `test_read_set_floats_by_id`: Create element, set float set, call `read_set_floats_by_id("AllTypes", 1, "weights")`, assert sorted values
- `test_read_set_date_time_by_id`: Use a schema with a string set that contains ISO datetime strings, or skip if no suitable schema exists (read_set_date_time_by_id wraps read_set_strings_by_id + datetime parsing). If testing: create element, set string set with datetime strings, call `read_set_date_time_by_id`, assert parsed datetimes are correct.

**Step 3: Add to `bindings/python/tests/test_database_update.py`:**

- `test_update_vector_strings`: Open all_types.sql, create element, call `update_vector_strings("AllTypes", 1, "labels", ["alpha", "beta"])`, read back, verify
- `test_update_set_integers`: Create element, call `update_set_integers("AllTypes", 1, "codes", [10, 20, 30])`, read back, verify (sort for comparison)
- `test_update_set_floats`: Create element, call `update_set_floats("AllTypes", 1, "weights", [1.1, 2.2])`, read back, verify

**Step 4: Check `test_database_create.py` and `test_database_csv.py` for completeness.**

Per research, Python is missing:
- `create_element` with time series data: Only add if the Python `Element.set()` supports time series row data. Check the Python Element class. If `set()` doesn't support time series values (likely -- time series are updated via `update_time_series_group`), skip and document.
- `create_element` with FK labels: Only add if the Python binding supports FK label resolution. Check existing Julia/Dart create tests for pattern. If Python handles it the same way, add a test using `relations.sql`.
- `read_all_vectors_by_id` with data (currently only tests empty case): Add a test that creates an element with actual vector data, then calls `read_all_vectors_by_id` and verifies the result dict has data.
- `read_all_sets_by_id` with data: Same pattern.

Add these tests to the appropriate existing files.

**Step 5: Run Python tests.**

```bash
bindings/python/test/test.bat
```

Verify all tests pass.
  </action>
  <verify>
Run `bindings/python/test/test.bat` -- all tests pass with zero failures.
  </verify>
  <done>
Python test_database_read_vector.py has 2-3 new tests. test_database_read_set.py has 4-5 new tests. test_database_update.py has 3 new tests. Convenience method coverage improved. All Python tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fill Lua test gaps and run full parity validation</name>
  <files>
    tests/test_lua_runner.cpp
  </files>
  <action>
**Step 1: Add gap-fill Lua tests to `tests/test_lua_runner.cpp`.**

Follow the existing pattern: `TEST(LuaRunner, TestName)` using googletest, create `Database` from schema, create `LuaRunner` with the db reference, then `lua.run(R"(...)")` with Lua script. Lua scripts use `db:method_name()` syntax. Use `all_types.sql` schema.

Add these tests:

- `ReadVectorStringsFromLua`: Create DB with all_types.sql, create element, update string vector from C++, then in Lua: `local vals = db:read_vector_strings_by_id("AllTypes", "labels", 1)` and assert `#vals == 2` and values match (use `assert(vals[1] == "alpha")`)
- `ReadSetIntegersFromLua`: Create element with integer set, Lua reads: `local vals = db:read_set_integers_by_id("AllTypes", "codes", 1)`, assert count
- `ReadSetFloatsFromLua`: Create element with float set, Lua reads: `local vals = db:read_set_floats_by_id("AllTypes", "weights", 1)`, assert count
- `ReadSetIntegersBulkFromLua`: Create elements, set integer sets, Lua calls `db:read_set_integers("AllTypes", "codes")`, assert result is table
- `ReadSetFloatsBulkFromLua`: Same pattern for float sets
- `ReadVectorStringsBulkFromLua`: Same pattern for string vectors
- `UpdateSetIntegersFromLua`: Create element, Lua calls `db:update_set_integers("AllTypes", 1, "codes", {10, 20, 30})`, then read back in Lua and assert
- `UpdateSetFloatsFromLua`: Same pattern for float set update
- `DescribeFromLua`: Open basic.sql, Lua calls `db:describe()`, assert no error

Check the Lua binding source (`src/lua_runner.cpp` or equivalent) to verify that these methods are actually bound in Lua. If any method is NOT bound in Lua, skip that test and note it in the summary. Do NOT add Lua bindings -- this is a test-only phase.

**Step 2: Build and run all 6 test suites.**

Execute in sequence:

```bash
# Build C++ and C API
cmake --build build --config Debug

# Run C++ tests
./build/bin/quiver_tests.exe

# Run C API tests
./build/bin/quiver_c_tests.exe

# Run Julia tests
bindings/julia/test/test.bat

# Run Dart tests
bindings/dart/test/test.bat

# Run Python tests
bindings/python/test/test.bat
```

All 6 must pass. If any failure, diagnose and fix within the gap-fill tests (do not modify existing tests or production code).

**Step 3: If `scripts/test-all.bat` exists, run it for final validation.**

```bash
scripts/test-all.bat
```
  </action>
  <verify>
1. `./build/bin/quiver_tests.exe` passes (includes Lua runner tests)
2. `./build/bin/quiver_c_tests.exe` passes
3. `bindings/julia/test/test.bat` passes
4. `bindings/dart/test/test.bat` passes
5. `bindings/python/test/test.bat` passes
6. All 6 test suites: zero failures
  </verify>
  <done>
test_lua_runner.cpp has up to 9 new gap-fill tests for Lua typed operations and describe. All 6 test suites (C++, C API, Julia, Dart, Python, Lua) run with zero failures. Test parity is achieved across all layers.
  </done>
</task>

</tasks>

<verification>
1. `bindings/python/test/test.bat` passes all tests including new gap-fill tests
2. `./build/bin/quiver_tests.exe` passes all tests including new Lua gap-fill tests
3. All 6 test suites (C++, C API, Julia, Dart, Python, Lua) produce zero failures
4. No production code was modified -- only test files changed
5. Python's 3-file read structure preserved (no merge)
</verification>

<success_criteria>
- Python has complete happy-path coverage for all typed read/update operations
- Lua has gap-fill tests for string vectors, integer/float sets, and describe
- All 6 test suites pass with zero failures (the final validation gate from CONTEXT.md)
- Test parity achieved: every functional area has tests in every layer that implements it
</success_criteria>

<output>
After completion, create `.planning/phases/07-test-parity/07-03-SUMMARY.md`
</output>
